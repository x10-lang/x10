\section{Related Work}
\label{sec:related}
%
Parallelizing BC became a key application for super computers to demonstrate
ever since the publication of the HPC Graph Analysis
Benchmarks~\cite{ssca_matlab}.
%
In this section, we will attempt to enumerate those efforts that are relevant
in that they deal with parallelism, scalability and betweenness centrality for
RMAT~\cite{Chakrabarti04:Recursive} graphs.
%
Bader et al.~\cite{Bader06:centrality} developed shared-memory parallel 
algorithms for various centrality metrics, including BC. 
%
In their work, they were able to compute BC metrics on graphs with $3$ million
nodes and $16$ million edges, and further claimed to be able to scale to
billions of nodes and edges.
%
In this work, they exploit fine-grained parallelism by involving multiple 
threads in finding shortest paths from a single source and coarse-grained
concurrency by starting shortest path searches from multiple sources 
simultaneously.
%
However, their approach does not exploit the vast amount of distributed-memory
parallelism that is on offer in most modern clusters and is not space
efficient.
%
Madduri et al.~\cite{Madduri:2009} presented a lock-free parallel algorithm for
computing BC, which significantly reduced synchronization overhead involved in
merging BC scores from various threads.
%
Their key idea was to replace predecessor set of a vertex $P_v(s)$ with a
successor set; this allows more parallelism as successor sets are private, as
opposed to predecessor sets, which have to be shared.
%
Using their approach, they were able to \textit{approximate} BC scores for a 
RMAT graph with billions of edges in a reasonable amount of time.
%
Like their previous efforts, they focussed exclusively on shared-memory
architectures.
%
Yang and Leonardi~\cite{Yang05} developed a distributed memory solution for 
BC, which achieves almost linear speedup (for at least 32 processors).
%
However, their approach relied on replicating the graph on all processors,
which is not space efficient and hence, not scalable.
%
Edmonds et al.~\cite{edmonds-hipc-2010} devised a space-efficient parallel 
BC computation targeted towards distributed-memory machines.
%
Their algorithm computes BC for both weighted and unweighted graphs, which are
partitioned \textit{a priori} with a space complexity of
$O(\lvert{}V\rvert{}+\lvert{}E\rvert{})$ building on the $\Delta{}$-stepping
single source shortest path solution by Meyer~\cite{Meyer03:DeltaStepping}.
%
Although Edmonds' algorithm does not preclude multi-level parallelism, it does 
not demonstrate multi-level parallelism.
%
Also, they achieved limited scaling for RMAT graphs --- scaling was
non-existant for over 16 processors.
%
Furthermore, all the efforts described above try to parallelize based on the
graph-traversal approach that was described in
Section~\ref{subsec:graph_traversal}.
%
It is important to note that there have been several noteworthy efforts to 
parallelize BFS traversal of a graph itself.
%
Notably, Agrawal et al.~\cite{virat_sc_2010} were able to achieve impressive 
numbers by exploiting the inherent non-uniform nature of modern multi-core 
processors.

%
% Now throw in stuff about C-BLAS
%
To our knowledge, there has been only one effort (Buluc~\cite{buluc-2010}) that
is based on the hybrid approach of using linear algebraic primitives for BFS
exploration (see Section~\ref{subsec:hybrid}).
%
Buluc successfully used BC as a case study for the need to have combinatorial
BLAS operations for graph algorithms by scaling up to 1225 processors on 
graphs with $32$ million vertices and $\approx{}256$ million edges.
%
Note that exact BC was not computed, but merely approximations were performed
by randomly picking certain vertices as source vertices.
%
This effort, however promising, still does not take advantage of the multiple
levels of fine-grained parallelism available on modern multi-core processors.


%~\cite{dijkstra59}, ~\cite{bellman58}, ~\cite{Meyer03:DeltaStepping},
%~\cite{meyer-diss}, , ~\cite{Frieze85},
%~\cite{Hassin85}, ~\cite{floyd62} , ~\cite{warshall62}, ~\cite{Blondel07},
%~\cite{Miller85}, ~\cite{clr90}, 
%~\cite{LogGP95}, ~\cite{Val90}, ~\cite{ParallelBGL}, ~\cite{boost},
%~\cite{Bader07:ApproxBC}, ~\cite{Sabidussi66}, ~\cite{Anthonisse71,Freeman77},
%~\cite{floyd62,JohnsonSSSP77,warshall62}, ~\cite{jaja92:intro_parallel_alg},
%~\cite{CrauserMehlhornMeyerSSSP98}, ~\cite{edmonds06:dimacs}
%~\cite{Madduri07:ALEXNEX}, ~\cite, 
%, ~\cite{Bollobas01:random_graphs},
%~\cite{CrauserMehlhornMeyerSSSP98}, ~\cite{Jenq87}, ~\cite{Kumar91},
%~\cite{Santos:2006}, 
