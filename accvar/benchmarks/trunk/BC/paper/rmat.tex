\section{RMAT Graphs}
\label{sec:rmat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
\SetKwFunction{rand}{rand}
\SetKwFunction{matrixy}{matrix}
\SetKwFunction{eltWiseMult}{eltWiseMult}
\SetKwFunction{eltWiseAdd}{eltWiseAdd}
\SetKwFunction{eltWiseNot}{eltWiseNot}
\SetKwFunction{sparse}{sparse}

\caption{RMAT}
\label{alg:rmat}
\KwIn{$n$: Number of nodes in graph $2^n$}
\KwIn{$k$: The total number of edges $(k\times{}2^{n})$}
\KwIn{$a$: Prob that an edge lies in first quadrant}
\KwIn{$b$: Prob that an edge lies in second quadrant}
\KwIn{$c$: Prob that an edge lies in third quadrant}
\KwIn{$d$: Prob that an edge lies in fourth quadrant}

$N\leftarrow{}2^{n},M\leftarrow{}k\times{}N$\;
$rowIndices\leftarrow{}colIndices\leftarrow{}$\matrixy{$1$,$M$,$1$}\;
$ab\leftarrow{}(a+b),cNorm\leftarrow{}\frac{c}{(c+d)},aNorm\leftarrow{}\frac{a}{(a+b)}$\;

\For{$i\in{}0..(n-1)$}{
  $lowerHalf\leftarrow{}$\rand{$M$,$1$}$>ab$\;
  $upperHalf\leftarrow{}$\eltWiseNot{$lowerHalf$}\;
  $cQuadrant\leftarrow{}$\eltWiseMult{$cNorm$,$lowerHalf$}\;
  $aQuadrant\leftarrow{}$\eltWiseMult{$aNorm$,$upperHalf$}\;
  $acQuadrant\leftarrow{}$\eltWiseAdd{$aQuadrant$,$cQuadrant$}\;
  $rowBits\leftarrow{}upperHalf$\;
  $colBits\leftarrow{}$\rand{$M$,$1$}$>acQuadrant$\;
  $currentRows\leftarrow{}$\eltWiseMult{$2^i$,$rowBits$}\;
  $currentCols\leftarrow{}$\eltWiseMult{$2^i$,$colBits$}\;
  $rowIndices\leftarrow{}rowIndices+currentRows$\;
  $colIndices\leftarrow{}colIndices+currentCols$\;
}

$rmat\leftarrow{}$\sparse{$rowIndices$,$colIndices$}\;
\Return{$rmat$}\;

\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}
%\begin{center}
%\begin{tabular}{|c|l|}  \hline
%Symbol & Meaning \\ \hline
%$N$ & Number of nodes in the real graph \\ \hline
%$2^n$ & Number of nodes in the generated graph \\ \hline
%$E$ & Number of generated edges (without duplicates) \\ \hline
%$(a,b,c,d)$ & Probabilities of an edge falling in each quadrant \\ 
% & $(a+b+c+d=1)$ \\ \hline
%\end{tabular}
%\caption{Table of symbols for RMAT graphs}
%\label{tbl:rmat}
%\end{center}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
As important as it is to be able to compute social networking metrics such as
BC accurately and efficiently, it is also necessary to be able to generate
synthetic graphs that mimic real-world graphs for testing and comparative
purposes.
%
In recent years, RMAT graphs~\cite{Chakrabarti04:Recursive} have been widely
adopted for generating synthetic graphs that mimic the power-law
characteristics demonstrated by several real-world graphs.
%
The idea behind RMAT graphs is simple: a graph $G(V,E)$ is considered to be 
a boolean adjacency matrix $A$, where $A_{ij}=1$ implies the prescence to an 
edge between vertices $(i,j)$.
%
The edge connections are determined using an appropriate psuedo-random number 
generator (PRNG) with range $[0,1)$ and four carefully chosen weights $a,b,c,$
and $d$ ($a+b+c+d=1$).
%
These four weights reflect the probability that any given edge falls within one
of the four equal sized partitions that the adjacency matrix is recursively
divided into.
%
Typically, $a\ge{}b$, $a\ge{}c$, $a\ge{}d$; in many real-world scenarios,
$(a+b)/(c+d)=3$.
%
Partitions $a$ and $d$ represent separate groups of nodes, and $b$ and $c$
represent cross-links between these groups.
%
Recursion ends upon reaching a $1\times{}1$ cell (at some position $(i,j)$,
which cannot be sub-divided any further.
%
If, by chance, that cell is already occupied, the duplicate is either
discarded.~\footnote{Weighted RMAT graphs are generated by accumulating
duplicate edges that fall in the same cell. For example, if an $l$ edges happen
to fall within the same cell $(i,j)$, the edge weight of $(i,j)$ is set to be
$l$.}
%
%Table~\ref{tbl:rmat} summarizes the various parameters involved in generating 
%an RMAT graph.
%
Algorithm~\ref{alg:rmat} lists the steps involved in generating an RMAT graph
using an array syntax; we now briefly discuss the method involved.
%
The input to the algorithm is $(n,k,a,b,c,d)$, and it outputs an adjacency 
matrix of a graph $G$ with $N=2^n$ vertices and $M=(k\times{}2^n)$ edges.
%
We iterate over $n$ passes determining $M$ separate edges during each
iteration; as mentioned before, duplicate edges are either discraded when 
creating the final adjacency matrix or are added up to represent edge weights.
%
As seen in lines $5$ and $6$, we generate $M$ random numbers and determine if
they are in the lower $(c+d)$ or the upper half $(a+b)$; these form the rows.
%
The columns are determined by another sample of $M$ random numbers; only this 
time, we chose to place them in either the $(a+c)$ half or $(b+d)$ half of the 
adjacency matrix.
%

%
% How can you parallelize this.
%
\subsubsection{Parallelization}
%
To parallelize algorithm~\ref{alg:rmat}, let us first understand how the edges 
are generated.
%
The \code{for} loop (lines $4$ to $15$) is repeated $n$ times, where $2^n$ is 
the number of vertices.
%
In each loop iteration, $M$ \code{boolean} values are generated for row numbers
($i$ in $a_{ij}$) and $M$ \code{boolean} values are generated for column
numbers ($j$ in $a_{ij}$), where $M=k\times{}2^n$ is the number of edges.
%
At iteration $l$, these \code{boolean} values are used to set the $l^{th}$ bit
of the $M$ row and column numbers that correspond to the $M$ edges.
%
At the end of $n$ iterations, each of the $n$ bits required to represent $2^n$
vertices are set to either $1$ or $0$ for each of the $M$ rows and columns.
%
There are two strategies for parallelization that can be pursued.
%
First, we can parallelize the execution of the $n$ iterations, taking care to
see that each parallel execution stream knows its global iteration numbers ---
this is necessary to ensure that edges generated connect vertices globally
rather than locally.
%
However, considering that we have only $n$ iterations (e.g., a graph with
$2^{64}$ nodes is an order of magnitude greater than \textit{exascale}), this
approach only gives us $n$-way parallelism.
%
A better approach is to parallelize across generation of the $M$ edges, which
gives us more parallelism; that is, each processor generates a portion of the 
graph.
%
However, since there might still be multiple edges, a global reduction
operation is required at the very end.
%
Note that if there is sufficient computation power, one can always parallelize
both the $n$ iterations and the generation of the $M$ edges.
