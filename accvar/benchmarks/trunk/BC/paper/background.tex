\section{Background}
\label{sec:background}
% Define BC
Betweenness Centrality (BC) is a graph theoretic metric of a node's importance
in a graph.~\footnote{In this paper, we focus exclusively on the unweighted,
directed graphs.}
%
Informally, a node's BC gives an indication of the ratio of the total number 
of shortest paths between all other nodes taken pair-wise.
%
That is, a node with a high centrality score can reach other nodes with 
relatively fewer hops than another node with a lower centrality score.
%
More formally, let $G(V,E)$ be a graph with the vertex set $V$ and edge set
$E$; let the number of vertices ($\lvert{}V\rvert{})$ be $n$ and the number of 
edges ($\lvert{}E\rvert{}$) be $m$.
%
The BC of a node (vertex) $v\in{V})$ is given by the equation:
%
\begin{equation}
BC_{v} = \sum_{s\ne{}v\ne{}t\in{}V}{\frac{\sigma{}_{st}(v)}{\sigma{}_{st}}}
\label{eq:bc}
\end{equation}
%
Where $\sigma{}_{st}$ is the number of shortest paths from node $s$ to node $t$ 
and $\sigma{}_{st}(v)$ is the number of those shortest paths that go through 
node $v$.
% Get into the simple means of computing BC
There are multiple ways of computing the BC scores of nodes in a graph; in the
following subsections, we highlight a few of these techniques.
%
Please note that regardless of the approach that is used in computing BC, the 
central kernel is enumerating all the shortest paths.

\subsection{Algebraic Approach}
\label{subsec:algebraic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.20\textwidth}
\begin{center}
\begin{displaymath}
\xymatrix{
a \ar[r] & d \ar[d] \\
b \ar[u] \ar[ur] \ar[r] & c \ar[ul]
}
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.18\textwidth}
\begin{center}
\begin{displaymath}
A = \left[ \begin{array}{cccc}
  0 & 0 & 0 & 1 \\
  1 & 0 & 1 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{A sample directed, unweighted graph and its resulting adjacency
matrix. A $1$ in position $a_{ij}$ indicates an edge from node $i$ to node
$j$.}
\label{fig:sample}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Every graph $G(V,E)$ can be represented as an adjacency matrix $A$, where an 
entry $a_{ij}$ is marked as $1$ \textit{iff} $(i,j)\in{}E$.
%
Figure~\ref{fig:sample} shows a sample four node graph and its related 
adjacency matrix; we will be using this graph as a running example throughout
this section.
%
By inspection, we can tell that the diameter of the graph is $3$; therefore, 
all the possible paths in the matrix (including the number of paths) can be 
enumerated by simply taking the powers of the adjacency matrix $A$; in other 
words, we find the transitive closure of the graph $G$.
%
However, computing the transitive closure is an over-kill; what we want in 
order to compute BC are just the shortest paths between all pair of vertices,
not all paths of length diameter or less.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^2 = \left[ \begin{array}{cccc}
   0 & 0 & 1 & 0 \\
   1 & 0 & 1 & 1 \\
   0 & 0 & 0 & 1 \\
   1 & 0 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\hspace{10pt}
\begin{minipage}{0.2\textwidth}
\begin{center}
\begin{displaymath}
A^3 = \left[ \begin{array}{cccc}
   1 & 0 & 0 & 0 \\
   1 & 0 & 1 & 1 \\
   0 & 0 & 1 & 0 \\
   0 & 0 & 0 & 1 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\caption{The $2^{nd}$ and $3^{rd}$ powers of the adjacency matrix, $A$, shown
in Figure~\ref{fig:sample}. $A^2$ shows the paths of path length $2$, and $A^3$
shows the paths in the sample graph of path length $3$.}
\label{fig:sample_powers}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Floyd-Warshall's Algorithm
\begin{algorithm}
\SetKwFunction{edgeCost}{edgeCost}
\SetKwFunction{minimum}{min}

\caption{FloydWarshall}
\label{alg:floyd_warshall}
\KwIn{$A$: Adjacency matrix of graph $G(V,E)$}

\For{$i=1:n$}{
  \For{$j=1:n$}{
    $path_{ij}$ = \edgeCost{$i$,$j$};\
  }
}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $path_{ij}$ = \minimum{$path_{ij}$, $path_{ik}+path_{kj}$};\
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Another possible solution was proposed by Batagelj~\cite{Batagelj-1994} and 
involved modifying Floyd/Warshall's algorithm~\cite{floyd62,warshall62} for 
all pairs shortest paths.
%
Briefly, Floyd/Warshall's algorithm computes all pairs shortest paths given an
adjacency matrix representation of a weighted graph in $O(n^3)$ time; the
algorithm is outlined in
Algorithm~\ref{alg:floyd_warshall}.~\footnote{Floyd/Warshall's does not handle
negative cycles, although it can be used to detect such cycles in a graph.}
%
In his solution, Batagelj avoided unnecessary work by using \textit{geodetic
semiring}, an instance of the closed semiring generalization for shortest
paths~\cite{Aho-1974}.
%
We briefly sketch the solution here for the sample graph shown in
Figure~\ref{fig:sample}.
%
First, from the adjacency matrix $A$, we create a new relation matrix
$R=[(d_{u,v}, n_{u,v})]$, where $d$ is the geodesic between $(u,v)\in{}E$ and
$n_{u,v}$ is the number of geodesics between $u$ and $v$; initially
%
\begin{equation}
(d,n)_{u,v} = \left\{ \begin{array}{rcl} 
  (1,1) & \mbox{for} & (u,v)\in{} A \\
  (\infty{},0) & \mbox{for} & (u,v)\notin{} A \\
\end{array}\right.
\label{eq:dnuv}
\end{equation}

Using this transformation on the adjacency matrix shown in
Figure~\ref{fig:sample}, we get the following matrix:
%
\begin{displaymath}
R = \left[ \begin{array}{cccc}
  (\infty{},0) & (\infty{},0) & (\infty{},0) & (1,1) \\
  (1,1) & (\infty{},0) & (1,1) & (1,1) \\
  (1,1) & (\infty{},0) & (\infty{},0) & (\infty{},0) \\
  (\infty{},0) & (\infty{},0) & (1,1) & (\infty{},0) \\
\end{array} \right]
\end{displaymath}
%
From this matrix $R$, we compute the geodesic closure $R^+$ using the semiring
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$, where:

\begin{equation}
(a,i)\oplus{}(b,j) = (min (a,b), \left\{ \begin{array}{rr} 
  i & a<b \\
  i+j & a=b \\
  j & a>b \\
\end{array}\right.)
\label{eq:oplus}
\end{equation}

\begin{equation}
(a,i)\odot{}(b,j) = (a+b, i\times{}j)
\label{eq:odot}
\end{equation}
%
The key intuition here is that knowing the distance ($d_{u,v}$), and the number
of shortest paths ($n_{u,v}$, it is easy to compute the number of shortest
paths between $(u,v)$ using the following equation:
%
\begin{equation}
n_{u,v}(t) = \left\{ \begin{array}{rr} 
  n_{u,t}\times{}n_{t,v} & d_{u,t}+d_{t,v} \\
  0 & otherwise \\
\end{array}\right.
\label{eq:bcoft}
\end{equation}
%
For a complete proof of $(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ being a 
geodesic semiring, please refer to Batagelj~\cite{Batagelj-1994}.~\footnote{
$(R, \oplus{}, \odot{}, (\infty,0), (0,1))$ is a semiring \textit{iff} all 
distances ($d_{i,j}$) are positive.}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Modified Batagelj's Algorithm
\begin{algorithm}
\SetKwFunction{minimum}{min}

\caption{computeGeodeticSemiRing}
\label{alg:batagelj}
\KwIn{$R$: Relational matrix of graph $G(V,E)$}

\For{$k=1:n$}{
  \For{$i=1:n$}{
    \For{$j=1:n$}{
      $distance$ = \minimum{$\infty{},d_{i,k}+d_{k,j}$}\;
      \If{$d_{i,j}\ge{}distance$}{
        $count = n_{i,k}\times{}n_{k,j}$\;
        \If{$d_{i,j}==distance$}{
          $n_{i,j} = n_{i,j} + count$\;
        }\Else{
          $n_{i,j} = count$\;
          $d_{i,j} = distance$\;
        }
      }
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The modified Floyd-Warshall's algorithm that computes $R^+$ is given in
Algorithm~\ref{alg:batagelj}; after application of this algorithm to the 
matrix $R$, we get:
%
\begin{displaymath}
R^+ = \left[ \begin{array}{cccc}
  (3,1) & (\infty{},0) & (2,1) & (1,1) \\
  (1,1) & (\infty{},0) & (1,1) & (1,1) \\
  (1,1) & (\infty{},0) & (3,1) & (2,1) \\
  (2,1) & (\infty{},0) & (1,1) & (3,1) \\
\end{array} \right]
\end{displaymath}
%
From $R^+$, it is simple to compute the BC of any node using
equations~\ref{eq:bcoft} and~\ref{eq:bc}.
%
For example, BC($d$) in $G$ (from Figure~\ref{fig:sample}) is $1$ as it lies 
on the only shortest path between $c$ and $a$.
%
In this method, computing $R^+$ takes $O(n^3)$ operations, and then computing
BC of any node requires considering all pair-wise entries, which takes a
further $O(n^2)$ computations.
%
Therefore, the overall computational complexity of the algorithm is $O(n^3)$;
space complexity is $O(n^2)$.
%
This is too steep a price to pay in real-world graphs, which are sparse and 
large.

%
% Brandes' approach
%
\subsection{Graph Traversal Approach}
\label{subsec:graph_traversal}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Brandes' Algorithm
\begin{algorithm}
\SetKwFunction{enqueue}{enqueue}
\SetKwFunction{dequeue}{dequeue}
\SetKwFunction{push}{push}
\SetKwFunction{pop}{pop}
\SetKwFunction{new}{new}
\SetKwFunction{append}{append}
\SetKwFunction{neighbor}{neighbor}

\caption{brandesBC}
\label{alg:brandes}
\KwIn{$G(V,E)$: A graph}
$BC_v = 0$, $v\in{}V$\;

\For{$s\in{}V$}{
  $S \leftarrow{}$ \new{$stack$}\;
  $P_w \leftarrow{} \emptyset{}, w\in{}V$\;
  $\sigma{}_w \leftarrow{} 0, w\in{}V; \sigma{}_s \leftarrow{} 1$\;
  $D_w \leftarrow{} -1, w\in{}V; D_s \leftarrow{} 0$\;
  $Q \leftarrow{}$ \new{$queue$}\;
  \enqueue{$Q$,$s$}\;

  \While{$Q\ne\emptyset{}$}{
    $v \leftarrow{}$ \dequeue{$Q$}\;
    \push{$S$,$v$}\;
    \ForEach{$w\leftarrow{}$\neighbor{$v$}}{
      \If{$D_w<0$}{
        \enqueue{$Q$,$w$}\;
        $D_w = D_v + 1$\;
      }
      \If{$D_w=D_v+1$}{
        \append{$P_w$,$v$}\;
        $\sigma{}_w = \sigma{}_w + \sigma{}_v$\;
      }
    }
  }
  $\delta{}_v \leftarrow{} 0, v\in{}V$\;
  \While{$S\ne\emptyset{}$}{
    $w \leftarrow{}$ \pop{$S$}\;
    \ForEach{$v\in{}P_w$}{
      $\delta{}_v\leftarrow{}\delta{}_v+\frac{\sigma{}_v}{\sigma{}_w}\times{}(1+\delta{}_w)$\;
    }
    \If{$w\ne{}s$}{
      $BC_w \leftarrow{} BC_w + \delta{}_w$\;
    }
  }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The key to computing efficiently is to exploit the sparsity of the graph 
structure (as opposed to its adjacency matrix).
%
This was the central theme on which Brandes~\cite{brandes01:_mathsoc} algorithm
is based.
%
Brandes recognized the recursive nature of BC computations that were exploited
in equation~\ref{eq:bcoft}.
%
For unweighted graphs, the basic idea is to perform breadth-first searches
(BFS) from all nodes.
%
At each step, the closest set of vertices are added, and during this step, 
cumulative betweenness scores for all vertices are computed by using the 
predecessor relationship.
%
Formally, let us define the \textit{predecessors} of a vertex $v$ on shortest
paths from $s$ to be
%
\begin{equation}
P_v(s)=\{u\in{}V:{u,v}\in{}E,d_G(s,v)=d_G(s,u)+(u,v)\}
\end{equation}
%
Where $d_G(s,v)$ is the shortest path from $s$ to $v$.
%
Now, the number of shortest paths from $s$ to $v$ ($\sigma{}_{sv}$) is exactly
$1$ more than the sum of the number of shortest paths from $s$ to each vertex
$u\in{}P_v(s)$. 
%
\begin{equation}
\sigma{}_{sv} = 1 + \sum_{u\in{}P_v(s)}\sigma{}_{su}
\end{equation}
%
Therefore, in $O(m)$ time, we can compute all the shortest paths and number of
shortest paths from a vertex $s$ to every other vertex; that is, in $O(mn)$,
we can compute all pairs and number of shortest paths.
%
Furthermore, this solution only requires $O(m+n)$ space.
%
The only thing left to do is to determine the contributions to the BC of each
vertex from every other vertex.
%
This information is already embedded in $P_v(s)$; there are
$\lvert{}P_v(s)\rvert{}$ distinct predecessors in the shortest paths from $s$
to $v$, and the number of shortest paths that each predecessor $P_{u,s}(i)$
is on is $\sigma{}_{su}(i)$. 
%
Now, using this information, we can compute the contribution of $(s,v)$ to 
the BC scores of each of the predecessors in $P_v(s)$.
%
\begin{equation}
BC(u) = BC(u) + \frac{\sigma{}_{su}}{\sigma{}_{sv}}, u\in{}P_v(s)
\end{equation}
%
In actual computation, the total number of shortest paths that go from $s$ to 
any vertex $v\in{}V$ through vertex $t\ne{}s,v$ is accumulated for every vertex
$s,v,t$ and finally, the BC scores are computed.
%
This final algorithm is given in Algorithm~\ref{alg:brandes}.

%
Let us consider the execution of Brandes's algorithm~\ref{alg:brandes} on our
sample graph $G$ from Figure~\ref{fig:sample}; let the start vertex be $a$.

\begin{align*}
Initialize\ BCs\ (line\ 1)\\
BC_a\leftarrow{}BC_b\leftarrow{}BC_c\leftarrow{}BC_d\leftarrow{}0\\
Initialize\ for\ BFS\ from\ a\ (lines\ 3\ to\ 8) \\ 
Q \leftarrow{} a, S \leftarrow{} \emptyset{} \\
P_a\leftarrow{}P_b\leftarrow{}P_c\leftarrow{}P_d\leftarrow{}\emptyset{}\\
\sigma{}_b\leftarrow{}\sigma{}_c\leftarrow{}\sigma{}_d\leftarrow{}0, \sigma{}_a\leftarrow{}1\\
D_b\leftarrow{}D_c\leftarrow{}D_d\leftarrow{}-1, D_a\leftarrow{}0
\end{align*}

\begin{align*}
BFS\ from\ a\ (lines\ 9\ to\ 18)\\
Q \leftarrow{} \emptyset{}, S \leftarrow{} (c,d,a) \\
P_a\leftarrow{}P_b\leftarrow{}\emptyset{}, P_c\leftarrow{}d,P_d\leftarrow{}a\\
\sigma{}_b\leftarrow{}0,\sigma{}_c\leftarrow{}\sigma{}_d\leftarrow{}\sigma{}_a\leftarrow{}1\\
D_b\leftarrow{}-1,D_c\leftarrow{}2,D_d\leftarrow{}1,D_a\leftarrow{}0
\end{align*}

\begin{align*}
Compute\ BC\ contributions\ (lines\ 19\ to\ 25)\\
S\leftarrow{}\emptyset{}\\
\delta{}_a\leftarrow{}2,\delta{}_d\leftarrow{}1,\delta{}_b\leftarrow{}\delta{}_c\leftarrow{}0\\
BC_a\leftarrow{}BC_b\leftarrow{}BC_c\leftarrow{}0,BC_d\leftarrow{}1
\end{align*}
%
This change in the $BC_d$ denotes that $d$ is on \textit{all} shortest paths
from $a$ to $d$.
%
Similarly, when shortest paths from $c$ are computed, the BC's are changed 
to:
%
\begin{align*}
BC_b\leftarrow{}BC_c\leftarrow{}0,BC_a\leftarrow{}BC_d\leftarrow{}1
\end{align*}
%
This change in the $BC_a$ denotes that $a$ is on \textit{all} shortest paths
from $c$ to $d$.
%
Similarly, when shortest paths from $d$ are computed, the BC's are changed 
to:
%
\begin{align*}
BC_b\leftarrow{}0,BC_a\leftarrow{}BC_c\leftarrow{}BC_d\leftarrow{}1
\end{align*}
%
This change in the $BC_c$ denotes that $c$ is on \textit{all} shortest paths
from $d$ to $a$.
%
Finally, when shortest paths from $b$ are computed, BC's do not change as 
$b$ is directly connected to $a,c,$ and $d$.

%
% Discuss some parallelization issues
%
\subsubsection{Parallelization}
%
\todoanju{Add information about parallelization.}

%
% Vector formulation of the problem
%
\subsection{Hybrid Formulation}
\label{subsec:hybrid}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Hybrid Algorithm
\begin{algorithm}
\SetKwFunction{minimum}{min}
\SetKwFunction{matrixy}{matrix}
\SetKwFunction{nnzExists}{nnzExists}
\SetKwFunction{matMult}{matMult}
\SetKwFunction{eltWiseAdd}{eltWiseAdd}
\SetKwFunction{eltWiseMult}{eltWiseMult}
\SetKwFunction{eltWiseAssign}{eltWiseAssign}
\SetKwFunction{eltWiseInvert}{eltWiseInvert}
\SetKwFunction{eltWiseNot}{not}

\caption{hybridBC}
\label{alg:hybrid}
\KwIn{$A$: Adjacency matrix of an unweighted graph $G$}
\KwIn{$n$: Dimension of $A$ ($n\times{}n$)}
\KwIn{$nVerts$: Number of BFS' to perform at once}
\tcp{We assume that nVerts is divisible by $n$}
$nPasses = \frac{nVerts}{n}$\;
$BC\leftarrow{}$\matrixy{$1$,$n$,$0$}\;

\ForEach{$p\in{}(1:nPasses)$}{
  $BFS \leftarrow{} \emptyset{}$\;
  $batch = ((p-1)\times{}nVerts+1):$\minimum{$p\times{}nVerts,N$}\;
  $nsp\leftarrow{}$\matrixy{$nVerts$,$n$, $0$}\;
  \ForEach{$row\in{}(1:nVerts)$}{$nsp(row,batch(row))\leftarrow{}1$\;}
  $depth\leftarrow{}0$\;
  \eltWiseAssign{$fringe$,$A(batch,:)$}\;

  \tcp{BFS search for all vertices in current batch}
  \While{\nnzExists{$fringe$}}{
    $depth\leftarrow{}depth+1$\;
    $nsp\leftarrow{}$\eltWiseAdd{$nsp$,$fringe$}\;
    $BFS(depth)\leftarrow{}fringe$\;
    $fringe\leftarrow{}$\matMult{$fringe$,$A$}\;
    \tcp{Reset entries for already visited vertices}
    $fringe\leftarrow{}$\eltWiseMult{$fringe$,\eltWiseNot{$nsp$}}\;
  }

  \tcp{Pre-compute BC updates for all but source vertices}
  $BC_{updt}\leftarrow{}$\matrixy{$1$,$n$,$1$}\;
  $nsp^{inv}\leftarrow{}$\eltWiseInvert{$nsp$}\;

  \tcp{Compute BC updates for all but source vertices}
  \For{$d\in{}(depth,depth-1,...,2)$}{
    \tcp{Compute child weights}
    $weights_1\leftarrow{}$\eltWiseMult{$BFS(d)$,$nsp^{inv}$}\;
    $weights\leftarrow{}$\eltWiseMult{$weights_1$,$BC_{updt}$}\;
    \tcp{Apply child weights}
    $temp_1\leftarrow{}$\matMult{$A$,$weights^T$}$^T$\;
    \tcp{Sum them up over parents}
    $temp_2\leftarrow{}$\eltWiseMult{$BFS(d-1)$,$nsp$}\;
    \tcp{Apply weights based on parents values}
    $temp_3\leftarrow{}$\eltWiseMult{$temp_1$,$temp_2$}\;
    $BC_{updt}\leftarrow{}$\eltWiseAdd{$BC_{updt}$,$temp_3$}\;
  }

  \tcp{Update BC scores from each source vertex's BFS}
  \For{$row\in{}(1:nVerts)$}{
    $BC\leftarrow{}$\eltWiseAdd{$BC$,$BC_{updt}(row,:)$}\;
  }
}
\tcp{Subtract additional values added by precomputation}
$BC\leftarrow{}$\eltWiseAdd{$BC$,\matrixy{$1$,$n$,$-nPasses$}}\;
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Both from a computational and spatial standpoint, there are several
inefficiencies in Algorithm~\ref{alg:brandes}.
%
First, notice that the queue $Q$ and the stack $S$ can be combined into one
array structure that when accessed from one end acts as a queue, and from the
other end, acts as a stack.
%
Second, the distance array $D$ is redundant in a BFS computation as
\textit{all} the shortest paths to a node must be reached during the same
\textit{BFS fringe's} expansion.
%
Any paths that reach a node during a later fringe are \textit{not} shortest
paths; hence, we can replace $D$ with a bit-array with one bit per node to
denote that its shortest path was reached during a particular fringe
expansion.
%
Third, consider the set of predecessors $P_v$ of a particular vertex $v$; since
$G$ is unweighted, these predecessors must have themselves been discovered 
during the previous fringe expansion.
%
That is, to compute the set of predecessors for a particular vertex $v$, we can
look at all its incoming \textit{discovered} edges; we need not store $P$ 
explicitly.
%
These three optimizations decrease the amount of space needed to execute
Brandes' algorithm by a significant constant factor.
%
The final optimization tries to perform BFS fringe expansions in terms of
sparse matrix multiplication with the \textit{fringe} vector (BLAS-2 kernel);
this allows exploration of one full fringe in one operation rather than
looping over all the neighbors of a particular node (lines $12$ to $18$ in
Algorithm~\ref{alg:brandes}).
%
The initial fringe vector contains just one entry, which corresponds to the 
start/source vertex.
%
Furthermore, shortest paths from multiple sources can be determined together by
replacing the sparse matrix vector product operation with a sparse matrix
matrix operation (BLAS-3 kernel); this is a standard trick in linear algebra
called \textit{blocking}.
%
However, note that the space requirements increase linearly with the block 
size.
%
After performing all these optimizations, we end up with
algorithm~\ref{alg:hybrid}, which is given as the sample MATLAB implementation
for the SSCA benchmarks~\cite{ssca_matlab}.
%
For example, in our sample graph $G$ from Figure~\ref{fig:sample}, the first 
fringe expansion starting out from vertices $a$ and $c$ can be expressed as
follows:

%
\begin{minipage}{0.18\textwidth}
\begin{center}
\begin{displaymath}
\left[ \begin{array}{cccc}
  a^T & b^T & c^T & d^T \\
  0 & 1 & 1 & 0 \\
  0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 1 \\
  1 & 1 & 0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\hspace{5pt}
\begin{minipage}{0.12\textwidth}
\begin{center}
\begin{displaymath}
\left[ \begin{array}{cc}
   a & c \\
   1 & 0 \\ 
   0 & 0 \\ 
   0 & 1 \\ 
   0 & 0 \\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\begin{minipage}{0.12\textwidth}
\begin{center}
\begin{displaymath}
 = \left[ \begin{array}{cc}
  d & a\\
  0 & 1\\
  0 & 0\\ 
  0 & 0\\
  1 & 0\\
\end{array} \right]
\end{displaymath}
\end{center}
\end{minipage}
\\ % Making sure we start on a new line. Minipage is awful!

%
For formatting purposes, we have shown right multiplication by the starting
vectors, which requires transposing the original adjacency matrix.
%
As can be seen, by starting out with BFS expansion from $a$ and $c$, we end 
up with the new fringe $d$ for $a$, and $a$ for $c$, respectively.
%
In fact, since this is always the case, we can forgo this computation and
simply consider the first fringe to be the starting vertex's adjacency row.
%
Please note that in an actual, high performance implementation, all matrices
are sparsely represented to save space.
%
Let us now work through Algorithm~\ref{alg:hybrid} for the sample graph $G$ 
from Figure~\ref{fig:sample}; for simplicity, we process only one vertex at a
time.
%
As before, let us start processing from vertex $a$.
%
\begin{align*}
(Initialization:\ lines\ 1\ to\ 10) \\
nPasses\leftarrow{}4,
BC\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 0 \\\end{array} \right]\\
BFS\leftarrow{}\emptyset{}, batch\leftarrow{}1:1, depth\leftarrow{}0\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 0 & 0 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 1 \\\end{array} \right]
\end{align*}
%
Here, we have initialized $nsp(a)$ to be $1$ and set the fringe to be $a$'s
adjacency ($d$).
%
\begin{align*}
(BFS\ search\ from\ a:\ lines\ 11\ to\ 16,\ first\ pass) \\
depth\leftarrow{}1,
BFS(1)\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 1 \\\end{array} \right]\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 0 & 1 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\\end{array} \right]
\end{align*}
%
At the end of the first pass, we discover vertex $c$; $nsp$ now indicates that
we have found one shortest path $(a,d)$.
%
\begin{align*}
(BFS\ search\ from\ a:\ lines\ 11\ to\ 16,\ second\ pass) \\
depth\leftarrow{}2,
BFS(2)\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\\end{array} \right]\\
nsp\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 1 & 1 \\\end{array} \right],
fringe\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 0 \\\end{array} \right]
\end{align*}
%
At the end of the second pass, we rediscover $a$, hence there is no new
$fringe$, and the BFS ceases; $nsp$ indicates that we discovered two shortest
paths, $(a,d)$ and $(c,d)$.
%
Now, we move on to updating the BC scores for all vertices based on our
exploration from vertex $a$.
%
\begin{align*}
(Initialize\ for\ BC\ updates:\ lines\ 17\ and \ 18) \\
BC_{updt}\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 1 \\\end{array} \right],
nsp^{inv}\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 1 & 1 \\\end{array} \right]
\end{align*}
%
As in Brandes~\ref{alg:brandes}), we move back from the final fringe to the 
first; in our example, this is $depth=2$.
%
\begin{align*}
(Compute\ and\ update\ child\ weights:\ lines\ 20\ to\ 25)\\
weights_1\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\\end{array} \right],
weights\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 1 & 0 \\\end{array} \right]\\
temp_1\leftarrow{}\left[\begin{array}{cccc}0 & 1 & 0 & 1 \\\end{array} \right],
temp_2\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 1 \\\end{array} \right]\\
temp_3\leftarrow{}\left[\begin{array}{cccc}0 & 0 & 0 & 1 \\\end{array} \right],
BC_{updt}\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 2 \\\end{array} \right]
\end{align*}
%
Next, we update the BC scores for each vertex with $BC_{updt}$.
%
\begin{align*}
(Update\ the\ BC\ scores:\ lines\ 26\ and\ 27)\\
BC\leftarrow{}\left[\begin{array}{cccc}1 & 1 & 1 & 2 \\\end{array} \right]
\end{align*}
%
Notice that in Algorithm~\ref{alg:hybrid}, we pre-compute $BC_{updt}$ to be $1$
for all vertices; hence, even though at the end of the BFS exploration for 
vertex $a$, BC scores of all vertices show a value $>0$, the only true change
is that of vertex $d$, which lies on the path from $a$ to $c$.
%
For brevity, we will now simply list the values of BC after the end of BFS 
exploration for each of the remaining vertices:
%
\begin{align*}
After\ BFS\ from\ b,\ 
BC\leftarrow{}\left[\begin{array}{cccc}2 & 2 & 2 & 3 \\\end{array} \right]\\
After\ BFS\ from\ c,\ 
BC\leftarrow{}\left[\begin{array}{cccc}4 & 3 & 3 & 4 \\\end{array} \right]\\
After\ BFS\ from\ d,\ 
BC\leftarrow{}\left[\begin{array}{cccc}5 & 4 & 5 & 5 \\\end{array} \right]
\end{align*}
%
The final trick to get the accurate BC scores is to subtract the number of
passes as each pass adds $1$ to each vertex's BC score
(Algorithm~\ref{alg:hybrid}, line $28$).
%
Note that without initializing $BC_{updt}$ with ones, line $24$ in
Algorithm~\ref{alg:hybrid} would not have computed $weights$ accurately;
therefore, it is critical to initialize $BC_{updt}$ with ones.
%
\begin{align*}
Subtract\ 4\ from\ BC\ (line\ 28):\ 
BC\leftarrow{}\left[\begin{array}{cccc}1 & 0 & 1 & 1 \\\end{array} \right]
\end{align*}
%
This gives the final and accurate BC scores for all the vertices.
%

%
% Discuss some parallelization issues
%
\subsubsection{Parallelization}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\centering
\begin{tabular}{|c|c|}
\hline
Variable & Space Requirement \\ \hline
$BC$ & $O(n)$ \\ \hline
$A$ & $O(m)$ \\ \hline
$fringe$ & $O(nVerts\times{}m)$ \\ \hline
$BFS$ & $O(depth\times{}nVerts\times{}m)$ \\ \hline
$nsp$ & $O(nVerts\times{}n)$ \\ \hline
$nsp^{inv}$ & Computed on the fly \\ \hline
$BC_{updt}$ & $O(n)$ \\ \hline
$weights_1$ & Computed on the fly \\ \hline
$weights$ & $O(n)$ \\ \hline
$temp_1,temp_2,temp_3$ & Computed ont the fly \\ \hline
\end{tabular}
\caption{Table depicting the space requirements of each variable in 
Algorithm~\ref{alg:hybrid}.}
\label{tbl:hybrid}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Algorithm~\ref{alg:hybrid} is expressed mostly in terms of computations on 
sparse matrices with sparse or dense vectors, which makes it especially 
suitable for parallelization.
%
In this segment, we discuss some of the obvious factors that affect parallel 
performance of Algorithm~\ref{alg:hybrid}.
%
First, let us recap that we are interested mostly in social network analysis
and the real-world graphs in this particular domain are similar to RMAT
graphs (see Section~\ref{sec:rmat}).
%
Briefly, these graphs follow the power-law, have low average connectivity and
low diameter.
%
First, let us recap that Algorithm~\ref{alg:hybrid} implements BFS as a
matrix-matrix multiplication (line $15$).
%
The \func{matMult} takes two matrices, $A$ and $fringe$; of these, $A$, the
adjacency matrix, is both constant and sparse.
%
On the other hand, $fringe$, might start out sparse depending on the start
vertices chosen for exploration, but becomes dense rapidly owing to the low
diameter of the graphs in question.
%
Table~\ref{tbl:hybrid} lists the space requirements of each of the variables 
used in Algorithm~\ref{alg:hybrid}.
%
Similarly, $BFS$, the array that stores the fringe at each depth-level also 
becomes dense rapidly.
%
For a large graph, $fringe$ and $BFS$ even for a single start vertex (i.e.,
$nVerts==1$) might not fit in memory.

%
% Discuss Approximate solutions 
%
\subsection{Approximating BC}
\label{subsec:approximate_bc}
%
Real-world graph sizes are ever increasing and BC computations are expensive;
consequently, approximate measures for BC have been explored.
%
There are three prominent works in this regard.
%
First, Eppstein and Wang~\cite{Eppstein-2004} describe a randomized
approximation algorithm for estimation of \textit{closeness centrality} in
weighted graphs.~\footnote{Closeness centrality (CC) of a vertex is a global
metric that measures the the distance of a vertex to all other vertices in the
graph.
%
Formally $CC_v=\frac{1}{\sum_{u\in{}V}{d(v,u)}}$.}
%
Their method (RAND) randomly chooses $k$ vertices one by one and use these 
vertices as start vertices for the single source shortest paths; that is, 
instead of solving all sources shortest paths problem, $k$ sources shortest
paths problem is solved.
%
They further proved that for $k=\Theta{(\frac{log(n)}{\epsilon{}^2})}$, their
algorithm approximates closeness centrality with an additive error of
$\epsilon{}\Delta{}_{G}$, where $\Delta{}_G$ is the diameter of the graph, and
$\epsilon{}$ is a small constant.
%
Brandes and Pich~\cite{brandes-2007}, after experimenting with various 
deterministic strategies for selecting source vertices for approximating BC,
concluded that a random sampling strategy was superior.
%
Bader et al.~\cite{Bader07:ApproxBC} presented an adaptive sampling technique
that approximates the BC of \textit{a given vertex}.
%
They conclude that for $0<\epsilon{}<\frac{1}{2}$, if the centrality of a vertex
$v$ is $\frac{n^2}{t}$ for some constant factor $t\ge{}1$, then with a 
probability $\ge{}(1-2\epsilon{})$, its centrality can be estimated within a 
factor of $\frac{1}{\epsilon{}}$ by using only $\epsilon{}t$ samples of 
source vertices.
